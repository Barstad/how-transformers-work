{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  TOKENIZATION IN TRANSFORMERS - \"HIDDEN\" FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the promises of deep learning in general is the ability to gobble up unstructured data and make sense of it, without the need for carefully crafted features. This is kind of true, as we need much less of it than in classical ML. BUT, its important to be aware of when/where feature engineering is happening, and the role it plays. One such place, which might be a bit overlooked, is in the tokenization process of transformer models.\n",
    "\n",
    "So what is a tokenizer really? Let's start by thinking about what we even do when we tokenize. When we tokenize, we are turning an object into a number. The object can be anything. It can be a word, a product in a shopping cart, a pixel in an image, etc. Two important properties of these object are as follows:\n",
    "\n",
    "1. The object is a single entity with inherent properties, and can be identified as such.\n",
    "2. The objects appear in a context, where they are connected to other objects. And their meaning is derived from the context in which they appear. \n",
    "\n",
    "Lets take an example:\n",
    "Consider a shopping cart with a few distinct objects: Flour, eggs, milk, sugar, butter, chocolate.\n",
    "Each of these objects have distinct qualities that make them recognizable and distinct from each other. One by one they can be eaten, or used in a wide variety of recipes. Together in this specific context, they can be used to bake a cake.  \n",
    "\n",
    "These two properties are very important, because of how these numbers are further processed by a transformer model. You see, when we input these numbers into a transformer, they are mapped into a high dimensional representation of the object, called an embedding. These embeddings are just vectors, one vector for each object we have defined. The nice thing about vectors is that they easily capture both similarity and context. And they can be manipulated by a deep learning model, like a transformer, to represent the meaning of the object, both in isolation and in the context of other objects.\n",
    "\n",
    "One famous and early use of tokenization into a vector embedding is the word2vec model. Simplified a bit, it worked by training a neural net to embed words into word-embeddings, and then predict the surrounding words in a window around a target word based on these words embeddings. This turned out to generate impressive representations of word semantics, and was an early example of self supervised learning. A lot has happened in NLP since then, but the idea of representing words as vectors is still a core concept. In this notebook, we will look at how we can use tokenization to turn text into numbers, and how these numbers can be used as input to a transformer model.\n",
    "\n",
    "Lets start by looking at a simple tokenization method in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: Goodbye World!\n",
      "tokens: [-1, 0, 0, 5, -1, -1, 8, 4, -1, 0, 1, 2, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# Given a sentence, we can easily split the text into tokens by doing this:\n",
    "data = \"Hello, world!\"\n",
    "characters = list(data)\n",
    "map = {char: i for i, char in enumerate(set(characters))}\n",
    "tokens = [map[char] for char in characters]\n",
    "\n",
    "\n",
    "new_sentence = \"Goodbye World!\"\n",
    "characters = list(new_sentence)\n",
    "tokens = [map.get(char, -1) for char in characters]\n",
    "\n",
    "print(\"sentence:\", new_sentence)\n",
    "print(\"tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o': 0, 'r': 1, 'l': 2, 'H': 3, ' ': 4, 'd': 5, '!': 6, 'w': 7, 'e': 8, ',': 9}\n"
     ]
    }
   ],
   "source": [
    "print(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just tokenized our dataset! We mapped the characters to numbers, and turned our sentence into a list of numbers. Great! However, this is not a very good tokenization method. We have a few problems, lets think back to the points we made earlier:\n",
    "\n",
    "Sure, each object here is a character, and therefore a distinct entity with a clear relation with surrounding characters. However, the complexity of each object is limited. A single character is not a complex object. It only contains a limited amount of information. If we encode only a limited amount of information for each object, we shift the burden on the model to infer the meaning primarily from the relationship between objects. This is not optimal. For a transformer model, we are better off with trying to encode more information in each object. How to do this? Just add more information to each object - more characters! This shifts us towards a more complex object, like a word. To be sure we capture the full complexity of the object we might need a larger vector to represent the object. Then our model can encode more information about the object, and makes it easier for the model to relate objects to each other.\n",
    "\n",
    "Another problem of this method is the vocabulary size. We only have 128 different characters in our vocabulary. When we encode text with this method, out tokenized text becomes very long. Becuase of how the transformer architecture is designed, it is important that the tokenized text is not too long. This is because every token looks at every other token in the sequence. This means quadratic complexity in the number of tokens. So, in short, shorter sequences are better. Just look at this python example for an illustration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize with chars: [3, 8, 2, 2, 0, 9, 4, 7, 0, 1, 2, 5, 6]\n",
      "tokenize with words: [1, 0]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_with_chars(sentence):\n",
    "    characters = list(sentence)\n",
    "    map = {char: i for i, char in enumerate(set(characters))}\n",
    "    tokens = [map.get(char, -1) for char in characters]\n",
    "    return tokens\n",
    "\n",
    "def tokenize_with_words(sentence):\n",
    "    words = sentence.split()\n",
    "    map = {word: i for i, word in enumerate(set(words))}\n",
    "    tokens = [map.get(word, -1) for word in words]\n",
    "    return tokens\n",
    "\n",
    "print(\"tokenize with chars:\", tokenize_with_chars(\"Hello, world!\"))\n",
    "print(\"tokenize with words:\", tokenize_with_words(\"Hello, world!\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This example highlights the role of feature engineering in NLP. It is almost decieving how such a small detail can have such a large impact on the performance of a model. We will now look at a more sophisticated method for tokenization, that addresses the problems we have identified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 995, 0]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tiktoken import encoding_for_model\n",
    "encoding = encoding_for_model(\"gpt-2\")\n",
    "print(encoding.encode(\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
